#!/usr/bin/env python3
# Copyright © Niantic, Inc. 2022.
import logging
import random
import time
from tqdm import *

import numpy as np
import torch
import torch.optim as optim
import torchvision.transforms.functional as TF
from torch.utils.data import DataLoader
from torch.utils.data import sampler

from dataset import CamLocDataset
import argparse
from distutils.util import strtobool
from pathlib import Path
import matplotlib.pyplot as plt
def _strtobool(x):
    return bool(strtobool(x))

if __name__ == '__main__':
    # Setup logging levels.
    logging.basicConfig(level=logging.INFO)

    # Setup logging levels.
    logging.basicConfig(level=logging.INFO)

    parser = argparse.ArgumentParser(
        description='Fast training of a scene coordinate regression network.',
        formatter_class=argparse.ArgumentDefaultsHelpFormatter)

    parser.add_argument('scene', type=Path,
                        help='path to a scene in the dataset folder, e.g. "datasets/Cambridge_GreatCourt"')

    parser.add_argument('--batch_size', type=int, default=5120,
                        help='number of patches for each parameter update (has to be a multiple of 512)')

    parser.add_argument('--use_half', type=_strtobool, default=True,
                        help='train with half precision')

    parser.add_argument('--use_homogeneous', type=_strtobool, default=True,
                        help='train with half precision')

    parser.add_argument('--use_aug', type=_strtobool, default=True,
                        help='Use any augmentation.')

    parser.add_argument('--aug_rotation', type=int, default=15,
                        help='max inplane rotation angle')

    parser.add_argument('--aug_scale', type=float, default=1.5,
                        help='max scale factor')

    parser.add_argument('--image_resolution', type=int, default=480,
                        help='base image resolution')

    # Clustering params, for the ensemble training used in the Cambridge experiments. Disabled by default.
    parser.add_argument('--num_clusters', type=int, default=None,
                        help='split the training sequence in this number of clusters. disabled by default')

    parser.add_argument('--cluster_idx', type=int, default=None,
                        help='train on images part of this cluster. required only if --num_clusters is set.')



    options = parser.parse_args()
    dataset = CamLocDataset(
        root_dir=options.scene / "train",
        mode=0,  # Default for ACE, we don't need scene coordinates/RGB-D.
        use_half=options.use_half,
        image_height=options.image_resolution,
        augment=options.use_aug,
        aug_rotation=options.aug_rotation,
        aug_scale_max=options.aug_scale,
        aug_scale_min=1 / options.aug_scale,
        num_clusters=options.num_clusters,  # Optional clustering for Cambridge experiments.
        cluster_idx=options.cluster_idx,    # Optional clustering for Cambridge experiments.
    )

    batch_generator = torch.Generator()
    batch_generator.manual_seed(1023)
    loader_generator = torch.Generator()
    loader_generator.manual_seed(1564)
    batch_sampler = sampler.BatchSampler(sampler.RandomSampler(dataset, generator=batch_generator),
                                                batch_size=2,
                                                drop_last=False)

    def seed_worker(worker_id):
                # Different seed per epoch. Initial seed is generated by the main process consuming one random number from
                # the dataloader generator.
                worker_seed = torch.initial_seed() % 2 ** 32
                np.random.seed(worker_seed)
                random.seed(worker_seed)

    training_dataloader = DataLoader(dataset=dataset,
                                        sampler=batch_sampler,
                                        batch_size=None,
                                        worker_init_fn=seed_worker,
                                        generator=loader_generator,
                                        pin_memory=True,
                                        num_workers=12,
                                        persistent_workers=12 > 0,
                                        timeout=60 if 12 > 0 else 0,
                                        )
    def tensor_to_image(tensor):
        """将PyTorch张量转换为Matplotlib可显示的格式"""
        if tensor.is_cuda:
            tensor = tensor.cpu()  # 转移到CPU
        # print(tensor)
        image = tensor.permute(1, 2, 0).numpy().astype(np.uint8)  # 调整维度顺序为 HWC
        # print(image.size)
        # 逆归一化（假设使用ImageNet归一化参数）
        return image  # 确保像素值在[0,1]范围
    def tensor_to_grayscale_image(tensor):
        # 1. 反向归一化: tensor = (original - mean) / std
        mean = torch.tensor([0.4])
        std = torch.tensor([0.25])
        tensor = tensor * std + mean  # 还原到 [0,1] 附近
        
        # 2. 裁剪到 [0,1] 范围（防止数值溢出）
        tensor = torch.clamp(tensor, 0, 1)
        
        # 3. 移除通道维度并转为 numpy
        image = tensor.squeeze(0).numpy()  # 形状 (H, W)
        return image

    for image_RGB,image_B1HW, image_mask_B1HW, gt_pose_B44, gt_pose_inv_B44, intrinsics_B33, intrinsics_inv_B33, _, _ in training_dataloader:
            # fig, axes = plt.subplots(1, 2, figsize=(15, 6))
            # print(image_RGB.shape,image_B1HW.shape)
            for i in range(image_RGB.size(0)):
                # 显示图像
                img_grey = tensor_to_grayscale_image(image_B1HW[i])
                plt.figure()
                plt.imshow(img_grey, cmap='gray', vmin=0, vmax=1)
                img = tensor_to_image(image_RGB[i])  # 转换张量格式
                plt.figure()
                plt.imshow(img)
                # axes[i].imshow(img)  # 确保像素值在[0,1]范围
                # axes[i].axis('off')
                # axes[i].set_title(f'Batch Image {i+1}') 
            plt.show()                 
            # 控制刷新逻辑
            user_input = input("按回车显示下一批，输入q退出: ")
            plt.close()  # 关闭当前图像释放内存

            if user_input.lower() == 'q':
                break
