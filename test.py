#!/usr/bin/env python3
# Copyright © Niantic, Inc. 2022.
import logging
import random
import time
import cv2
import matplotlib
from tqdm import *
from skimage import color
from skimage import io
from skimage.transform import rotate, resize

import numpy as np
import torch
import torch.optim as optim
import torchvision.transforms.functional as TF
from torch.utils.data import DataLoader
from torch.utils.data import sampler

from dataset import CamLocDataset
import argparse
from distutils.util import strtobool
from pathlib import Path
import matplotlib.pyplot as plt
def _strtobool(x):
    return bool(strtobool(x))

if __name__ == '__main__':
    # Setup logging levels.
    logging.basicConfig(level=logging.INFO)

    # Setup logging levels.
    logging.basicConfig(level=logging.INFO)

    parser = argparse.ArgumentParser(
        description='Fast training of a scene coordinate regression network.',
        formatter_class=argparse.ArgumentDefaultsHelpFormatter)

    parser.add_argument('scene', type=Path,
                        help='path to a scene in the dataset folder, e.g. "datasets/Cambridge_GreatCourt"')

    parser.add_argument('--batch_size', type=int, default=5120,
                        help='number of patches for each parameter update (has to be a multiple of 512)')

    parser.add_argument('--use_half', type=_strtobool, default=True,
                        help='train with half precision')

    parser.add_argument('--use_homogeneous', type=_strtobool, default=True,
                        help='train with half precision')

    parser.add_argument('--use_aug', type=_strtobool, default=True,
                        help='Use any augmentation.')

    parser.add_argument('--aug_rotation', type=int, default=15,
                        help='max inplane rotation angle')

    parser.add_argument('--aug_scale', type=float, default=1.5,
                        help='max scale factor')

    parser.add_argument('--image_resolution', type=int, default=480,
                        help='base image resolution')

    # Clustering params, for the ensemble training used in the Cambridge experiments. Disabled by default.
    parser.add_argument('--num_clusters', type=int, default=None,
                        help='split the training sequence in this number of clusters. disabled by default')

    parser.add_argument('--cluster_idx', type=int, default=None,
                        help='train on images part of this cluster. required only if --num_clusters is set.')



    options = parser.parse_args()
    dataset = CamLocDataset(
        root_dir=options.scene / "train",
        mode=1,  # Default for ACE, we don't need scene coordinates/RGB-D.
        use_half=options.use_half,
        image_height=options.image_resolution,
        augment=options.use_aug,
        aug_rotation=options.aug_rotation,
        aug_scale_max=options.aug_scale,
        aug_scale_min=1 / options.aug_scale,
        num_clusters=options.num_clusters,  # Optional clustering for Cambridge experiments.
        cluster_idx=options.cluster_idx,    # Optional clustering for Cambridge experiments.
    )

    batch_generator = torch.Generator()
    batch_generator.manual_seed(1023)
    loader_generator = torch.Generator()
    loader_generator.manual_seed(1564)
    batch_sampler = sampler.BatchSampler(sampler.RandomSampler(dataset, generator=batch_generator),
                                                batch_size=1,
                                                drop_last=False)

    def seed_worker(worker_id):
                # Different seed per epoch. Initial seed is generated by the main process consuming one random number from
                # the dataloader generator.
                worker_seed = torch.initial_seed() % 2 ** 32
                np.random.seed(worker_seed)
                random.seed(worker_seed)

    training_dataloader = DataLoader(dataset=dataset,
                                        sampler=batch_sampler,
                                        batch_size=None,
                                        worker_init_fn=seed_worker,
                                        generator=loader_generator,
                                        pin_memory=True,
                                        num_workers=12,
                                        persistent_workers=12 > 0,
                                        timeout=60 if 12 > 0 else 0,
                                        )
    def tensor_to_image(tensor):
        """将PyTorch张量转换为Matplotlib可显示的格式"""
        if tensor.is_cuda:
            tensor = tensor.cpu()  # 转移到CPU
        # print(tensor)
        image = tensor.permute(1, 2, 0).numpy().astype(np.uint8)  # 调整维度顺序为 HWC
        # print(image.size)
        # 逆归一化（假设使用ImageNet归一化参数）
        return image  # 确保像素值在[0,1]范围
    def tensor_to_grayscale_image(tensor):
        # 1. 反向归一化: tensor = (original - mean) / std
        mean = torch.tensor([0.4])
        std = torch.tensor([0.25])
        tensor = tensor * std + mean  # 还原到 [0,1] 附近
        
        # 2. 裁剪到 [0,1] 范围（防止数值溢出）
        tensor = torch.clamp(tensor, 0, 1)
        
        # 3. 移除通道维度并转为 numpy
        image = tensor.squeeze(0).numpy()  # 形状 (H, W)
        return image
    cmap = matplotlib.colormaps.get_cmap('Spectral_r')
    rootdir = options.scene / "train"
    coord_dir = rootdir / 'depth'
    coord_files = sorted(coord_dir.iterdir())
    depth = io.imread(coord_files[0])
    depth = depth.astype(np.float64)
    depth /= 1000  # from millimeters to meters
    depth = (cmap(depth)[:, :, :3] * 255)[:, :, ::-1].astype(np.uint8)
    cv2.namedWindow("Combined Image", cv2.WINDOW_NORMAL)
    cv2.imshow("Combined Image", depth)
    print(depth)
    key = cv2.waitKey(0)
    if key == 27:  
        cv2.destroyAllWindows()

    # for image_RGB,depth,image_B1HW, image_mask_B1HW, gt_pose_B44, gt_pose_inv_B44, intrinsics_B33, intrinsics_inv_B33, _, _ in training_dataloader:
    #         # fig, axes = plt.subplots(1, 2, figsize=(15, 6))
    #         # print(image_RGB.shape,image_B1HW.shape)
    #         # 深度图可视化
    #         print(depth)
            # pred_depth_np = pred_depth*255.0
            # pred_depth_np = pred_depth_np.detach().cpu().numpy().astype(np.uint8)
            # pred_depth_np = (cmap(pred_depth_np)[:, :, :3] * 255)[:, :, ::-1].astype(np.uint8)
            # depth_HW_np = depth_HW*255.0
            # depth_HW_np = depth_HW_np.detach().cpu().numpy().astype(np.uint8)
            # depth_HW_np = (cmap(depth_HW_np)[:, :, :3] * 255)[:, :, ::-1].astype(np.uint8)
            # cv2.namedWindow("Combined Image", cv2.WINDOW_NORMAL)
            # combined = np.hstack((pred_depth_np,depth_HW_np))
            # # 将拼接后的图像放大 2 倍
            # display_img = cv2.resize(combined, (combined.shape[1] * 4, combined.shape[0] * 4))
            # cv2.resizeWindow("Combined Image", combined.shape[1] * 4, combined.shape[0] * 4)
            # cv2.imshow("Combined Image", display_img)
            # cv2.imshow("Raw Image", image_BGR)
            # key = cv2.waitKey(0)
            # if key == 27:  
            #     cv2.destroyAllWindows()